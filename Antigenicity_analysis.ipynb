{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfasta import Fasta\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import use_CNN\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, confusion_matrix,mean_squared_error,matthews_corrcoef\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_selection import SelectFromModel, f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import metrics\n",
    "from keras import backend\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_load(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        x_train = pickle.load(f)\n",
    "        y_train = pickle.load(f)\n",
    "        x_test = pickle.load(f)\n",
    "        y_test = pickle.load(f)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PSO select CNN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## time range of data\n",
    "## training set: 1968 - 2010\n",
    "## test set: 2011 - 2016\n",
    "path = \"./data_1968-2010_2011-2016.pkl\"\n",
    "x_train, y_train, x_test, y_test = pickle_load(path)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this part runs for a very long time\n",
    "be prepared\n",
    "\"\"\"\n",
    "### ------------ PSO selection of optimal CNN ------------ ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "from keras import backend\n",
    "\n",
    "\n",
    "\n",
    "### helper function:\n",
    "## convert CNN vector between original form and [0,1]\n",
    "\n",
    "# Number of layers of CNN 1 4\n",
    "# Number of filters 32 256\n",
    "# Kernel size  2 5\n",
    "# Stride 1 3\n",
    "# Pooling size 1 4\n",
    "# Dropout probability 0.1 0.5\n",
    "# Number of dense layers 1 3\n",
    "# Number of neurons in dense layers 72 256\n",
    "def convert_cnn_vector(x):\n",
    "    ## number of dense layers could be smaller than 3\n",
    "    new_x = np.zeros(len(x))\n",
    "    ## convert normed to original vector\n",
    "    if x[0]<=1:\n",
    "\n",
    "        for i in range(0, 20, 5):\n",
    "            if x[i]>=0:\n",
    "                new_x[i] = 32 + np.round(x[i]*(256-32))\n",
    "                new_x[i+1] = 2 + np.round(x[i+1]*(5-2)) if x[i+1]>=0 else 2\n",
    "                new_x[i+2] = 1 + np.round(x[i+2]*(3-1)) if x[i+2]>=0 else 1\n",
    "                new_x[i+3] = 1 + np.round(x[i+3]*(4-1)) if x[i+3]>=0 else 1\n",
    "                new_x[i+4] = 0.1 + x[i+4]*(0.5-0.1) if x[i+4]>=0 else 0.1\n",
    "            else:\n",
    "                new_x[i] = 0\n",
    "                new_x[i+1] = 0\n",
    "                new_x[i+2] = 0\n",
    "                new_x[i+3] = 0\n",
    "                new_x[i+4] = 0\n",
    "        for i in range(20, len(x), 2):\n",
    "            if x[i] >= 0:\n",
    "                new_x[i] = 72 + np.round(x[i]*(256-72)) \n",
    "                new_x[i+1] = 0.1 + x[i+1]*(0.5-0.1) if x[i+1]>=0 else 0.1\n",
    "            else:\n",
    "                new_x[i] = 0\n",
    "                new_x[i+1] = 0\n",
    "    ## convert to normalized vector\n",
    "    else:\n",
    "        for i in range(0, 20, 5):\n",
    "            new_x[i] = (x[i] - 32)/(256-32) if x[i] != 0 else -0.5\n",
    "            new_x[i+1] = (x[i+1] - 2)/(5-2) if x[i+1] != 0 else -0.5\n",
    "            new_x[i+2] = (x[i+2] - 1)/(3-1) if x[i+2] != 0 else -0.5\n",
    "            new_x[i+3] = (x[i+3] - 1)/(4-1) if x[i+3] != 0 else -0.5\n",
    "            new_x[i+4] = (x[i+4] - 0.1)/(0.5-0.1) if x[i+4] != 0 else -0.5\n",
    "        for i in range(20, len(x), 2):\n",
    "            new_x[i] = (x[i] - 72)/(256-72) if x[i] != 0 else -0.5\n",
    "            new_x[i+1] = (x[i+1] - 0.1)/(0.5-0.1) if x[i+1] != 0 else -0.5\n",
    "    # print(\"x0: \",new_x[0])\n",
    "    ## return new and original\n",
    "    return(new_x)\n",
    "#### collection of objective functions ####\n",
    "def obj(x, trainx, trainy, testx, testy, ratio=0.0):\n",
    "    new_x = convert_cnn_vector(x)\n",
    "    \n",
    "    if not use_CNN.check_cnn_valid(new_x,trainx.shape[1:]):\n",
    "        new_x = use_CNN.random_generate_cnn(None, 1, n_cnn, n_dense, p_cnn, p_dense, trainx.shape[1:], fix_dense=fixed)\n",
    "        new_x = new_x.reshape(new_x.shape[1])\n",
    "    x = convert_cnn_vector(new_x)\n",
    "    ## get objective value\n",
    "    model = use_CNN.make_cnn(new_x, trainx.shape[1:])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(trainx, trainy, epochs=10, batch_size=600)\n",
    "    _, train_result = use_CNN.DL_mcc(model, trainx, trainy)\n",
    "\n",
    "    #pred_score = model.predict(testx)\n",
    "    _, test_result = use_CNN.DL_mcc(model, testx, testy)\n",
    "\n",
    "    return(train_result['MCC']*ratio + (1-ratio)*test_result['MCC'])\n",
    "def test_obj(x, trainx, trainy, testx, testy, ratio=0.4):\n",
    "    new_x = convert_cnn_vector(x)\n",
    "    if not use_CNN.check_cnn_valid(new_x,trainx.shape[1:]):\n",
    "        new_x = use_CNN.random_generate_cnn(None, 1, n_cnn, n_dense, p_cnn, p_dense, trainx.shape[1:], fix_dense=fixed)\n",
    "        new_x = new_x.reshape(new_x.shape[1])\n",
    "    x = convert_cnn_vector(new_x)\n",
    "    ## get objective value\n",
    "\n",
    "    model = use_CNN.make_cnn(new_x, trainx.shape[1:])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    ## no training\n",
    "    return(np.sum(new_x))\n",
    "\n",
    "### generate a vector of length dimension, with 0~1\n",
    "### n_output: number of output\n",
    "def initial_position_int(dimension, n_output, shape):\n",
    "    ini_pos = np.zeros((n_output, dimension))\n",
    "    for i in range(ini_pos.shape[0]):\n",
    "        x = use_CNN.random_generate_cnn(None, 1, n_cnn, n_dense, p_cnn, p_dense, shape, fix_dense=fixed)\n",
    "        # print(x[0])\n",
    "        norm_x = convert_cnn_vector(x[0])\n",
    "        if i > 0:\n",
    "            for j in range(0, i):\n",
    "                while np.allclose(norm_x, ini_pos[j,:]):\n",
    "                    x = use_CNN.random_generate_cnn(None, 1, n_cnn, n_dense, p_cnn, p_dense, shape, fix_dense=fixed)\n",
    "                    norm_x = convert_cnn_vector(x[0])\n",
    "        ini_pos[i,:] = norm_x\n",
    "    return(ini_pos)\n",
    "\n",
    "#### ---------------- neighbor function ------------------- ####   \n",
    "def defign_von_neumann_neighbor(swarm):\n",
    "    r = 0\n",
    "    c = 0\n",
    "    l = len(swarm)\n",
    "    l_sqrt_int = int(np.floor(np.sqrt(l)))\n",
    "    for i in range(l_sqrt_int, 0, -1):\n",
    "        if l % i == 0:\n",
    "            r = i\n",
    "            c = int(l/r)\n",
    "            break\n",
    "    ## print(\"r \", r, \"; c \", c)\n",
    "    ## neighborhood mesh\n",
    "    #print(\"-- mesh dim: \", r, \", \", c)\n",
    "    mesh = np.zeros((r,c))\n",
    "    check_mesh = []\n",
    "    for i in range(l):\n",
    "        col = int((i+1)%c - 1)\n",
    "        row = int(np.ceil((i+1)/c) - 1)\n",
    "        mesh[row][col] = i    ### use location to find idx of swarm\n",
    "        check_mesh.append((row, col))   ### use idx to find location\n",
    "    return(mesh, check_mesh)\n",
    "\n",
    "def find_local_best_von_neumann(swarm, idx, mesh, check_mesh, length):\n",
    "    row, col = check_mesh[idx]\n",
    "    local_best = -1     ## local best\n",
    "    locao_best_pos = []  ## local best location\n",
    "    ### go through the neighbor of idx\n",
    "    for i in range(row - length, row + length +1):\n",
    "        for j in range(col - length, col + length + 1):\n",
    "            if i < 0: \n",
    "                i += mesh.shape[0]\n",
    "            elif i >= mesh.shape[0]:\n",
    "                i = i - mesh.shape[0] \n",
    "            if j <0:\n",
    "                j += mesh.shape[1]\n",
    "            elif j >= mesh.shape[1]:\n",
    "                j = j - mesh.shape[1]\n",
    "            dist = np.absolute(row + col - i - j) \n",
    "            \n",
    "            ## update local best\n",
    "            if dist <= length:\n",
    "                target = int(mesh[i][j])\n",
    "                if local_best < swarm[target].best_value:\n",
    "                    local_best = swarm[target].best_value\n",
    "                    local_best_pos = swarm[target].pos_best.copy()\n",
    "    return(local_best_pos, local_best)\n",
    "\n",
    "### ------------------- Particle ------------------ ###\n",
    "class Particle:\n",
    "    def __init__(self,x0):\n",
    "        self.position=[]          # particle position, binary vector\n",
    "        self.velocity=[]          # particle velocity\n",
    "        self.pos_best=[]          # best position individual\n",
    "        self.best_value=-1          # best error individual\n",
    "        self.value=-1               # error individual\n",
    "        \n",
    "        \n",
    "        for i in range(0, len(x0)):\n",
    "            self.velocity.append(random.uniform(-1,1))\n",
    "            self.position.append(x0[i])\n",
    "\n",
    "    # evaluate current fitness\n",
    "    def evaluate(self, costFunc, xtrain, ytrain, xtest, ytest, tv_ratio):\n",
    "        self.value = costFunc(self.position, xtrain, ytrain, xtest, ytest, tv_ratio)\n",
    "\n",
    "        # check to see if the current position is an individual best\n",
    "        if self.value > self.best_value or self.best_value==-1:\n",
    "            self.pos_best=self.position\n",
    "            self.best_value=self.value\n",
    "\n",
    "    # update new particle velocity\n",
    "    def update_velocity(self, best_global_pos):\n",
    "        w=0.5       # constant inertia weight (how much to weigh the previous velocity)\n",
    "        c1=2        # cognative constant\n",
    "        c2=2        # social constant\n",
    "        # w1 = 0.9\n",
    "        # w2 = 0.4\n",
    "        # w = (w1-w2)*(maxiter - currentiter)/maxiter + w2\n",
    "        for i in range(0, len(self.position)):\n",
    "            r1 = random.random()\n",
    "            r2 = random.random()\n",
    "\n",
    "            vel_cognitive = c1*r1*(self.pos_best[i]-self.position[i])\n",
    "            vel_social = c2*r2*(best_global_pos[i]-self.position[i])\n",
    "            self.velocity[i] = w*self.velocity[i]+vel_cognitive+vel_social\n",
    "\n",
    "    # update the particle position based off new velocity updates\n",
    "    def update_position(self, dimension):\n",
    "        tmp_pos = np.array(self.position) + np.array(self.velocity)\n",
    "        tmp_pos[tmp_pos>1] = 1\n",
    "        tmp_pos[tmp_pos<0] = 0\n",
    "        self.position=tmp_pos\n",
    "        \n",
    "#### ---------------- PSO main function ------------------- ####\n",
    "\n",
    "def PSO_feature_selection(costFunc, dimension, xtrain, ytrain, xtest, ytest, tv_ratio, num_particles = 25, \n",
    "                          maxiter = 30, verbose = False, use_neighbor = True):\n",
    "    length = 1        # neighborhood dist to be checked\n",
    "    best_value = -1   # best global value/MCC\n",
    "    best_pos = []     # best global position\n",
    "    best_trend = []   # the trend of best value over iteration\n",
    "    \n",
    "    ## ---- initialize position of particles ---- ##\n",
    "    \n",
    "    ini_pos = initial_position_int(dimension, num_particles, xtrain.shape[1:])\n",
    "    swarm = []\n",
    "    for i in range(num_particles):\n",
    "        swarm.append(Particle(ini_pos[i]))\n",
    "        \n",
    "    ## ---- index to find neighbor ---- ##\n",
    "    mesh, check_mesh = defign_von_neumann_neighbor(swarm)\n",
    "    \n",
    "    ## ---- optimize ---- ##\n",
    "    for i in range(maxiter):\n",
    "        # cycle through particles in swarm and evaluate fitness\n",
    "        for j in range(0,num_particles):\n",
    "            swarm[j].evaluate(costFunc, xtrain, ytrain, xtest, ytest, tv_ratio)\n",
    "\n",
    "            # determine if current particle is the best (globally)\n",
    "            if not use_neighbor:\n",
    "                if swarm[j].value > best_global_value or best_global_value == -1:\n",
    "                    best_pos = swarm[j].position.copy()\n",
    "                    best_value = float(swarm[j].value)\n",
    "                \n",
    "        # cycle through swarm and update velocities and position\n",
    "        for j in range(0,num_particles):\n",
    "            if use_neighbor:\n",
    "                best_pos, best_value = find_local_best_von_neumann(swarm, j, mesh, check_mesh, length)\n",
    "            swarm[j].update_velocity(best_pos)\n",
    "            swarm[j].update_position(dimension)\n",
    "            \n",
    "        tmp_v = -1\n",
    "        for j in range(0,num_particles):    \n",
    "            if swarm[j].best_value > tmp_v:\n",
    "                tmp_v = swarm[j].best_value\n",
    "        best_trend.append(tmp_v)\n",
    "        if i>=45 and len(best_trend)>2:\n",
    "            if np.absolute(best_trend[-1] - best_trend[-2]) < 0.000001:\n",
    "                break\n",
    "        if verbose:\n",
    "            print(\"-- PSO iteration: \", i)\n",
    "            print(\"---- Global best value: \", tmp_v)\n",
    "            if i % 10 ==0:\n",
    "                util.send_email(\"Iteration report\", str(i))\n",
    "            # for j in range(num_particles):\n",
    "            #     print(\"------ Swarm \", j, \", best: \", swarm[j].best_value, \", value: \", swarm[j].value)\n",
    "        ## clear session for every iteration\n",
    "        backend.clear_session()\n",
    "\n",
    "    best_pos = []\n",
    "    for i in range(len(swarm)):\n",
    "        if swarm[i].best_value == best_trend[-1]:\n",
    "            best_pos = swarm[i].pos_best.copy()\n",
    "            break\n",
    "    \n",
    "    return(best_trend, best_pos)\n",
    "\n",
    "\n",
    "def test_convert_cnn_vector():\n",
    "    norm_x = [0.5, 0.2, -0.2, 0.4, 0.1, 0.5, 0.2, 0.3, 0.4, 0.1, 0.5, 0.2, 0.3, 0.4, 0.1, 0, 0, 0, 0, 0, 0.2, 0.4, 0.1, 0.3, 0.6,0.2]\n",
    "    #x = [70, 4, 1, 3, 0.4, 210, 5, 2, 3, 0.45, 210, 5, 2, 2, 0.45, 0, 0, 0, 0, 3, 100, 0.5, 112, 0.2, 128, 0.3]\n",
    "    norm_x = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0.4, 0.1, 0.3, 0.6,0.2]\n",
    "    \n",
    "    print(norm_x)\n",
    "    org_x = convert_cnn_vector(norm_x)\n",
    "    print(\"org x: \", org_x)\n",
    "    norm_x2 = convert_cnn_vector(org_x)\n",
    "    print(\"norm x 2: \",norm_x2)\n",
    "    org_x2 = convert_cnn_vector(norm_x2)\n",
    "    print(\"org x2: \", org_x2)\n",
    "    print(np.allclose(org_x, org_x2, rtol = 1e-4))\n",
    "    print(\"Check valid: \",use_CNN.check_cnn_valid(norm_x, [100,116,1]))\n",
    "def test_random_cnn_struc():\n",
    "    struc = use_CNN.random_generate_cnn(1234567890, 1, 4, 3, 5, 2, [10,10,1], fix_dense=False)\n",
    "    print(struc.shape)\n",
    "    # print(struc[0][0:20])\n",
    "    print(struc[0][0:20].reshape(4,5))\n",
    "    print(struc[0][20:].reshape(3,2))\n",
    "def test_make_cnn():\n",
    "    x = np.array([70, 4, 1, 0.4, 3, 210, 5, 2, 0.45, 3, 210, 5, 2, 0.45, 3, 0, 0, 0, 0, 3, 100, 0.5, 112, 0.2])\n",
    "    x = [0.5, 0.2, 0.3, 0.4, 0.1, 0.5, 0.2, 0.3, 0.4, 0.1, 0.5, 0.2, 0.3, 0.4, 0.1, 0, 0, 0, 0, 0, 0.2, 0.4, 0.1, 0.3]\n",
    "    new_x = convert_cnn_vector(x)\n",
    "    new_x = use_CNN.random_generate_cnn(None, 1, 4, 3, 5, 2, [100,10,1], fix_dense=False)\n",
    "    new_x = new_x[0]\n",
    "    print(new_x[0:20].reshape(4,5))\n",
    "    model = use_CNN.make_cnn(new_x, [100,10,1])\n",
    "    print(model.summary())\n",
    "def declair_global_variable():\n",
    "    global n_cnn, n_dense, p_cnn, p_dense, fixed, seed\n",
    "    n_cnn, n_dense, p_cnn, p_dense, fixed, seed = 4, 3, 5, 2, True, 1489683273\n",
    "    print(\"Len of vec: \", n_cnn*p_cnn+n_dense*p_dense)\n",
    "def read_support(file):\n",
    "    opt_pos = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            opt_pos = list(map(int, line.split(\"\\t\")))\n",
    "    f.close()\n",
    "    return(np.array(opt_pos, dtype=bool))\n",
    "\n",
    "def main(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    ## training/ validation ratio\n",
    "    tv_ratio = 0.0\n",
    "    declair_global_variable()\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "\n",
    "    best_trend, best_pos = PSO_feature_selection(obj, n_cnn*p_cnn+n_dense*p_dense, \n",
    "        x_train, y_train, x_test, y_test, tv_ratio, maxiter = 50, verbose = True)\n",
    "    orig_best_pos = convert_cnn_vector(best_pos)\n",
    "\n",
    "\n",
    "    best_trend_file = \"./output/PSO_CNN_struc_best_trend_sort3d.txt\"\n",
    "    best_pos_file = \"./output/PSO_CNN_struc_best_pos_sort3d.txt\"\n",
    "    \n",
    "    FOUT = open(best_trend_file, \"w\")\n",
    "    FOUT.write(\"\\t\".join(str(x) for x in best_trend))\n",
    "    FOUT.close()\n",
    "    \n",
    "    FOUT = open(best_pos_file, \"w\")\n",
    "    FOUT.write(\"\\t\".join(str(x) for x in orig_best_pos))\n",
    "    FOUT.close()\n",
    "\n",
    "main(x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train a model with early stop\n",
    "## \n",
    "## perc: positive case change by perc, negative case change by -perc\n",
    "def cust_rebalance(y_train, train_weight, perc=0.1):\n",
    "    p = np.bincount(y_train)\n",
    "    weights, counts = np.unique(train_weight, return_counts=True)\n",
    "    print(\"Initial weights:\", weights)\n",
    "\n",
    "    for i, item in enumerate(p):\n",
    "        idx = counts == item\n",
    "        if i == 0:\n",
    "            trainw_idx = train_weight == weights[idx]\n",
    "            train_weight[trainw_idx] = weights[idx]*(1-perc)\n",
    "        elif i==1:\n",
    "            trainw_idx = train_weight == weights[idx]\n",
    "            train_weight[trainw_idx] = weights[idx]*(1+perc)\n",
    "    print(\"Final weights:\", np.unique(train_weight, return_counts=False))\n",
    "\n",
    "    \n",
    "def run_early_stop(struc_file, x_train, y_train, x_test, y_test, epoch = 50):\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "\n",
    "    ## read CNN structure\n",
    "    model = use_CNN.read_cnn_struc(struc_file, x_train.shape[1:], regression=False)\n",
    "\n",
    "    # optimizer = optimizers.SGD(lr=0.01, decay=1e-8, momentum=1, nesterov=True)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    ## compute sample weight \n",
    "    train_weight = compute_sample_weight(\"balanced\",y_train)\n",
    "    valid_weight = compute_sample_weight(\"balanced\",y_test)\n",
    "    # rebalance(train_weight, valid_weight)\n",
    "\n",
    "    cust_rebalance(y_train, train_weight, perc=0.4)\n",
    "    ## checkpoint            \n",
    "    filename=\"CNN_model_{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    filepath = os.path.join(\"./output/model\",filename)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # fit\n",
    "    history = model.fit(x_train, y_train, epochs=epoch, batch_size=600, sample_weight=train_weight, \n",
    "        callbacks=callbacks_list, validation_split=0.2)\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    _, train_result = use_CNN.DL_mcc(model, x_train,y_train)\n",
    "    _, test_result = use_CNN.DL_mcc(model, x_test,y_test)\n",
    "    \n",
    "    \n",
    "struc_file = \"./PSO_CNN_struc_best_pos_2018-3-22_artificial96.txt\"\n",
    "run_early_stop(struc_file, x_train, y_train, x_test, y_test, epoch = 200)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train CNN model for additional cases\n",
    "\"\"\"\n",
    "paths = [\"./data_1968-1997_1998.pkl\",\n",
    "        \"./data_1968-1997_1998.pkl\",\n",
    "        \"./data_1968-1998_1999.pkl\",\n",
    "        \"./data_1968-1999_2000.pkl\",\n",
    "        \"./data_1968-2000_2001.pkl\",\n",
    "        \"./data_1968-2001_2002.pkl\",\n",
    "        \"./data_1968-2002_2003.pkl\",\n",
    "        \"./data_1968-2003_2004.pkl\",\n",
    "        \"./data_1968-2004_2005.pkl\",\n",
    "        \"./data_1968-2005_2006.pkl\",\n",
    "        \"./data_1968-2006_2007.pkl\",\n",
    "        \"./data_1968-2007_2008.pkl\",\n",
    "        \"./data_1968-2008_2009.pkl\",\n",
    "        \"./data_1968-2009_2010.pkl\",\n",
    "        \"./data_1968-2010_2011.pkl\"]\n",
    "\n",
    "x_train, y_train, x_test, y_test = pickle_load(paths[0])\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "run_early_stop(struc_file, x_train, y_train, x_test, y_test, epoch = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
